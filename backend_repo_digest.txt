Repository Source Code Contents
Generated on: Fri Aug  8 21:37:36 IST 2025
----------------------------------------
File: ./domains/data_science/prompts/toc_prompts.py
----------------------------------------
# ============================================================================
# File: domains/data_science/prompts/toc_prompts.py
# Generic prompt that passes user preferences to LLM without bias
# ============================================================================

# Single generic prompt that lets LLM decide based on user preferences
DATA_SCIENCE_TOC_PROMPT = """
Create a comprehensive Data Science curriculum Table of Contents based on the user's specific preferences and background.

User's Complete Profile:
{user_preferences}

Instructions:
1. Analyze the user's goals, experience level, learning style, time constraints, and technical background
2. Design a curriculum that EXACTLY matches their stated preferences
3. If they want career-focused content, emphasize job-ready skills
4. If they want hands-on learning, prioritize practical projects over theory
5. If they're a beginner, start with fundamentals; if advanced, focus on sophisticated topics
6. Respect their time constraints and daily commitment preferences
7. Build on their existing technical skills (programming, math, domain knowledge)

Generate topics that cover relevant areas of data science:
- Programming and tools
- Mathematics and statistics  
- Data manipulation and analysis
- Machine learning
- Data visualization
- Specialized domains (NLP, computer vision, etc.)
- Industry applications
- Portfolio/project work

Structure the curriculum to:
- Have logical progression and clear prerequisites
- Include realistic time estimates based on their availability
- Mark essential vs optional topics based on their goals
- Provide detailed subtopics that show learning value
- Match the complexity to their experience level

Let the user's stated preferences guide ALL decisions about:
- Depth vs breadth
- Theory vs practice ratio
- Beginner vs advanced content
- Career vs academic focus
- Time allocation per topic

Domain: data_science
"""

def get_toc_prompt(user_preferences: dict) -> str:
    """
    Format the generic prompt with user preferences
    No selection logic - just pass everything to LLM
    """
    # Convert user preferences to a readable format for the LLM
    formatted_preferences = []
    
    for key, value in user_preferences.items():
        if isinstance(value, list):
            formatted_preferences.append(f"- {key.replace('_', ' ').title()}: {', '.join(value) if value else 'None specified'}")
        else:
            formatted_preferences.append(f"- {key.replace('_', ' ').title()}: {value}")
    
    preferences_text = '\n'.join(formatted_preferences)
    
    return DATA_SCIENCE_TOC_PROMPT.format(user_preferences=preferences_text)
----------------------------------------
File: ./models.py
----------------------------------------
# ============================================================================
# File: models.py (NEW FILE)
# Enhanced Pydantic models for onboarding data
# ============================================================================

from pydantic import BaseModel
from typing import Dict, Any, List, Optional
from enum import Enum

class LearnerLevel(str, Enum):
    COMPLETE_BEGINNER = "complete-beginner"
    BEGINNER = "beginner"
    INTERMEDIATE = "intermediate"
    ADVANCED = "advanced"

class CourseStyle(str, Enum):
    HANDS_ON = "hands-on"
    BALANCED = "balanced"
    CONCEPT_HEAVY = "concept-heavy"

class TotalTime(str, Enum):
    FOUR_HOURS = "4-hours"
    ONE_TWO_WEEKS = "1-2-weeks"
    ONE_MONTH = "1-month"
    TWO_THREE_MONTHS = "2-3-months"

class DailyTime(str, Enum):
    THIRTY_MIN = "30-min"
    ONE_HOUR = "1-hour"
    TWO_THREE_HOURS = "2-3-hours"

class TechnicalBackground(BaseModel):
    programming: List[str] = []
    math: List[str] = []
    domain: List[str] = []

class OnboardingData(BaseModel):
    goal: Optional[str] = None
    learner_level_detailed: Optional[LearnerLevel] = None
    course_material_detailed: Optional[CourseStyle] = None
    total_time_detailed: Optional[TotalTime] = None
    daily_time_detailed: Optional[DailyTime] = None
    technical_background: Optional[TechnicalBackground] = None
----------------------------------------
File: ./models/toc_models.py
----------------------------------------
# ============================================================================
# File: models/toc_models.py
# Pydantic models for TOC and learning path structures
# ============================================================================

from pydantic import BaseModel, Field
from typing import List, Optional, Dict, Any
from enum import Enum

class DifficultyLevel(str, Enum):
    BEGINNER = "beginner"
    INTERMEDIATE = "intermediate"
    ADVANCED = "advanced"

class TopicType(str, Enum):
    THEORETICAL = "theoretical"
    PRACTICAL = "practical"
    MIXED = "mixed"

class SubTopic(BaseModel):
    id: str = Field(..., description="Unique identifier for the subtopic")
    name: str = Field(..., description="Name of the subtopic")
    description: str = Field(..., description="Brief description of what this subtopic covers")
    estimated_hours: float = Field(..., description="Estimated time to complete in hours")
    difficulty: DifficultyLevel = Field(..., description="Difficulty level of this subtopic")
    prerequisites: List[str] = Field(default=[], description="List of prerequisite subtopic IDs")

class Topic(BaseModel):
    id: str = Field(..., description="Unique identifier for the topic")
    name: str = Field(..., description="Name of the topic")
    description: str = Field(..., description="Brief description of what this topic covers")
    estimated_hours: float = Field(..., description="Total estimated time for this topic in hours")
    difficulty: DifficultyLevel = Field(..., description="Overall difficulty level")
    topic_type: TopicType = Field(..., description="Type of topic (theoretical/practical/mixed)")
    subtopics: List[SubTopic] = Field(default=[], description="List of subtopics under this topic")
    prerequisites: List[str] = Field(default=[], description="List of prerequisite topic IDs")
    is_core: bool = Field(..., description="Whether this topic is core/essential for the domain")

class TableOfContents(BaseModel):
    domain: str = Field(..., description="The domain this TOC is for (e.g., 'data_science', 'cat_exam')")
    title: str = Field(..., description="Title of the course/curriculum")
    description: str = Field(..., description="Brief description of the overall curriculum")
    total_estimated_hours: float = Field(..., description="Total estimated time for all topics")
    topics: List[Topic] = Field(..., description="List of all topics in the curriculum")
    learning_path_suggestions: List[str] = Field(
        default=[], 
        description="Suggested topic IDs for different learning paths"
    )

class LearningPath(BaseModel):
    user_id: str = Field(..., description="User identifier")
    session_id: str = Field(..., description="Session identifier")
    domain: str = Field(..., description="Domain for this learning path")
    selected_topics: List[str] = Field(..., description="List of selected topic IDs")
    estimated_total_hours: float = Field(..., description="Total estimated hours for selected topics")
    created_at: str = Field(..., description="When this learning path was created")
    user_preferences: Optional[Dict[str, Any]] = Field(
        default=None, 
        description="User preferences from onboarding"
    )

# Response models for API endpoints
class TOCResponse(BaseModel):
    success: bool = Field(default=True)
    data: TableOfContents = Field(..., description="Generated table of contents")
    message: str = Field(default="TOC generated successfully")

class LearningPathResponse(BaseModel):
    success: bool = Field(default=True)
    data: LearningPath = Field(..., description="Created learning path")
    message: str = Field(default="Learning path created successfully")
----------------------------------------
File: ./prompts.py
----------------------------------------
# ============================================================================
# File: prompts.py 
# Enhanced prompts that use rich onboarding data
# ============================================================================

COURSE_STRUCTURE_PROMPT = """
You are an expert data science educator. Create a course structure for: {topic}

Student Profile:
- Primary Goal: {goal}
- Experience Level: {level} (Detailed: {detailed_level})
- Learning Style: {style} (Detailed: {detailed_style})
- Total Time Available: {total_hours} hours ({detailed_total_time})
- Daily Commitment: {daily_minutes} minutes ({detailed_daily_time})

Technical Background Assessment:
Programming Experience: {programming_skills}
Mathematics Background: {math_background}  
Domain Knowledge: {domain_knowledge}

Based on this detailed profile:
1. Leverage their existing technical skills
2. Fill identified knowledge gaps
3. Adapt content complexity to their experience
4. Align projects with their stated goal
5. Structure modules to fit their time constraints

Create a modular course where each module is 30-60 minutes.

Return JSON:
{{
  "title": "Personalized Course Title",
  "total_modules": number,
  "student_profile_summary": "Brief summary of their background and goals",
  "learning_path_rationale": "Why this path was chosen for them",
  "modules": [
    {{
      "id": "mod_1",
      "title": "Module Title",
      "learning_objectives": ["objective1", "objective2"],
      "topics": ["topic1", "topic2"],
      "estimated_minutes": 45,
      "difficulty_level": "beginner|intermediate|advanced",
      "prerequisites_covered": ["skill1", "skill2"],
      "aligns_with_goal": "How this module helps their stated goal"
    }}
  ],
  "personalization_notes": {{
    "skipped_basics": ["concept1", "concept2"],
    "emphasis_areas": ["area1", "area2"],
    "recommended_pace": "Based on their time commitment"
  }}
}}
"""

MODULE_CONTENT_PROMPT = """
Create a {style} lesson for: {module_title}

Student Context:
- Goal: {goal}
- Experience Level: {level}
- Programming Skills: {programming_skills}
- Math Background: {math_background}
- Previous Knowledge: {domain_knowledge}

Module Details:
- Topics to cover: {topics}
- Learning objectives: {objectives}
- Target duration: {duration} minutes
- Difficulty level: {difficulty_level}

Personalization Instructions:
- Skip basics they already know: {skip_basics}
- Emphasize: {emphasis_areas}
- Connect to their goal: {goal_connection}

Format:
1. Start with personalized introduction acknowledging their background
2. Include {theory_percent}% theory and {code_percent}% hands-on code
3. Use examples relevant to their goal ({goal})
4. Add 2-3 exercises matching their programming level
5. Provide "next steps" that build toward their goal

Structure your response as:
## Personalized Introduction
[Acknowledge their background and connect to goals]

## Core Concepts  
[Theory explanation adapted to their math/programming level]

## Practical Application
[Code examples using libraries they're comfortable with]

## Hands-on Exercises
[2-3 exercises matching their skill level]

## Goal Connection
[How this module advances their stated goal]
"""

EXERCISE_GENERATOR_PROMPT = """
Create a Python coding exercise for: {topic}

Student Profile:
- Programming Level: {programming_level}
- Math Background: {math_level}
- Goal: {goal}
- Previous Domain Experience: {domain_experience}

Exercise Requirements:
- Difficulty: {difficulty}
- Focus: {focus_area}
- Time to complete: ~{estimated_minutes} minutes

Personalization:
- Use libraries they're familiar with: {familiar_libraries}
- Avoid concepts they haven't learned: {avoid_concepts}
- Connect to their goal: {goal}

Return JSON:
{{
  "title": "Exercise Title (Goal-Aligned)",
  "description": "What student needs to do (with goal context)",
  "starter_code": "def function_name():\\n    # Your code here\\n    # Hint: Use {familiar_concept} that you learned in {previous_experience}\\n    pass",
  "hints": [
    "Beginner-friendly hint based on their background",
    "Progressive hint building on their {programming_level} skills", 
    "Advanced hint connecting to their {goal}"
  ],
  "solution": "Complete solution with comments explaining concepts",
  "test_cases": [
    {{"input": "test input", "expected": "expected output"}},
    {{"input": "edge case relevant to {goal}", "expected": "expected output"}}
  ],
  "learning_notes": "What they should learn from this exercise",
  "goal_connection": "How this exercise helps achieve their {goal}"
}}
"""

COACH_HINT_PROMPT = """
Student Profile:
- Goal: {goal} 
- Experience: {experience_level}
- Programming Background: {programming_background}
- Math Background: {math_background}

Current Situation:
- Working on: {exercise_title}
- Their code: {user_code}
- Error (if any): {error}
- Attempt number: {attempt}
- Time spent: {time_spent} minutes

Coaching Instructions:
1. Reference their background knowledge positively
2. Connect hints to concepts they already understand
3. If attempt > 3, provide more specific guidance
4. Always encourage and relate back to their goal
5. Use their programming experience level to calibrate hint complexity

Provide a helpful, personalized hint that:
- Acknowledges their {experience_level} level
- Builds on their {programming_background} background  
- Connects to their goal of {goal}
- Is appropriately technical for their skill level
"""
----------------------------------------
File: ./coach_service.py
----------------------------------------
# ============================================================================
# File: coach_service.py
# Enhanced coach service with personalization
# ============================================================================

from typing import Optional, List, Dict 
from backend.services.llm_service import LLMService
from prompts import COACH_HINT_PROMPT

class CoachService:
    def __init__(self, model: str = "openai/gpt-oss-20b:free"):
        self.llm = LLMService(model=model)
        self.motivation_messages = [
            "Great progress! You're {percent}% through this module!",
            "Keep going! Just {remaining} minutes left in this session.", 
            "Nice work! You've completed {count} exercises so far.",
            "You're doing great! Remember, every expert was once a beginner.",
            "Stuck? That's normal! Try breaking down the problem into smaller steps."
        ]
    
    def set_model(self, model: str):
        """Switch models for the coach service"""
        self.llm.set_model(model)

    def get_hint(self, exercise: Dict, user_code: str, attempt: int, 
                 student_context: Dict = None, error: Optional[str] = None) -> str:
        """Generate contextual hint using student background"""
        
        # Use student context if available, otherwise use defaults
        context = student_context or {
            'goal': 'general learning',
            'experience_level': 'beginner',
            'programming_skills': ['basic'],
            'math_background': ['high-school'],
            'time_spent': 0
        }
        
        prompt = COACH_HINT_PROMPT.format(
            goal=context.get('goal', 'general learning'),
            experience_level=context.get('experience_level', 'beginner'),
            programming_background=', '.join(context.get('programming_skills', [])) or 'basic',
            math_background=', '.join(context.get('math_background', [])) or 'high school level',
            exercise_title=exercise.get('title', ''),
            user_code=user_code,
            error=error or "No error",
            attempt=attempt,
            time_spent=context.get('time_spent', 0)
        )
        
        hint = self.llm.generate(
            "You are a supportive, personalized coding tutor. Adapt your teaching style to the student's background.",
            prompt,
            temperature=0.6
        )
        return hint
    
    def answer_question(self, question: str, context: Dict, student_context: Dict = None) -> str:
        """Answer conceptual questions with personalization"""
        
        student_info = ""
        if student_context:
            student_info = f"""
            Student Background:
            - Goal: {student_context.get('goal', 'general learning')}
            - Experience: {student_context.get('experience_level', 'beginner')}
            - Programming: {', '.join(student_context.get('programming_skills', [])) or 'basic'}
            """
        
        prompt = f"""
        {student_info}
        
        Current Context: {context.get('module_title', 'Unknown')}
        Student Question: {question}
        
        Provide a clear, personalized answer that:
        1. Matches their experience level
        2. References concepts they already know
        3. Connects to their learning goal
        4. Uses appropriate technical depth
        
        Keep it under 200 words and be encouraging.
        """
        
        answer = self.llm.generate(
            "You are a knowledgeable data science tutor who personalizes explanations.",
            prompt
        )
        return answer
    
    def get_motivation(self, progress: float, time_spent: int, exercises_done: int, 
                      student_context: Dict = None) -> str:
        """Generate motivational message with personalization"""
        
        goal = student_context.get('goal', 'learning') if student_context else 'learning'
        
        if progress < 0.3:
            return f"Great start on your {goal} journey! You're {int(progress*100)}% through this module."
        elif progress < 0.7:
            return f"Halfway there! You're making excellent progress toward your {goal} goal."
        elif progress < 0.9:
            return f"Almost done! Just a little more to master this concept for your {goal}."
        else:
            return f"Fantastic! You're about to complete this module. Your {goal} skills are growing!"
    
    def check_understanding(self, module_content: str, user_responses: List[str], 
                           student_context: Dict = None) -> Dict:
        """Personalized comprehension check"""
        
        context_info = ""
        if student_context:
            context_info = f"Student goal: {student_context.get('goal', 'learning')}, Experience: {student_context.get('experience_level', 'beginner')}"
        
        prompt = f"""
        {context_info}
        Module covered: {module_content[:500]}...
        Student responses to exercises: {user_responses}
        
        Based on their background and responses, assess:
        1. Understanding level (1-5) 
        2. Areas needing reinforcement
        3. Readiness for next concepts
        4. Personalized feedback for their goal
        
        Return JSON: {{"understanding": 1-5, "weak_areas": [], "feedback": "...", "next_recommendations": []}}
        """
        
        assessment = self.llm.generate_json(
            "Assess student understanding with personalized recommendations.",
            prompt
        )
        return assessment
----------------------------------------
File: ./test_openrouter.py
----------------------------------------
import requests
import json
import os
from dotenv import load_dotenv

load_dotenv()

def test_openrouter():
    # Try to get API key from environment first, then fallback to hardcoded
    OPENROUTER_API_KEY = os.getenv("OPENROUTER_API_KEY")
    
    if not OPENROUTER_API_KEY:
        # If no environment variable, ask for input
        print("ðŸ”‘ OpenRouter API Key not found in environment variables.")
        print("You can either:")
        print("1. Set environment variable: export OPENROUTER_API_KEY='your-key'")
        print("2. Enter it below (or edit the script)")
        print()
        OPENROUTER_API_KEY = input("Enter your OpenRouter API key: ").strip()
    
    if not OPENROUTER_API_KEY or OPENROUTER_API_KEY == "your-key-here":
        print("âŒ No valid API key provided!")
        print("Get your key from: https://openrouter.ai/keys")
        return
    
    print(f"ðŸ”‘ Using API key: {OPENROUTER_API_KEY[:10]}...{OPENROUTER_API_KEY[-4:]}")
    
    # Test with a simpler request first
    headers = {
        "Authorization": f"Bearer {OPENROUTER_API_KEY}",
        "Content-Type": "application/json"
    }
    
    # Simple test payload
    payload = {
        "model": "google/gemma-3n-e2b-it:free",
        "messages": [
            {"role": "user", "content": "Say hello"}
        ],
        "max_tokens": 50
    }
    
    print("ðŸ§ª Testing simple request first...")
    
    try:
        response = requests.post(
            "https://openrouter.ai/api/v1/chat/completions",
            headers=headers,
            json=payload,
            timeout=30
        )
        
        print(f"ðŸ“¡ Response Status: {response.status_code}")
        
        if response.status_code == 200:
            print("âœ… API Key works! Testing course generation...")
            test_course_generation(OPENROUTER_API_KEY)
        elif response.status_code == 401:
            print("âŒ Invalid API key!")
            print("Response:", response.json())
            print("\nðŸ”§ Troubleshooting:")
            print("1. Make sure you copied the FULL key from https://openrouter.ai/keys")
            print("2. The key should start with 'sk-or-v1-'")
            print("3. Try creating a new key")
        elif response.status_code == 429:
            print("âš ï¸ Rate limited - but API key works!")
            print("Free model limits: 20 req/min, 50 req/day")
        else:
            print(f"âŒ Error: {response.status_code}")
            print("Response:", response.text)
            
    except Exception as e:
        print(f"ðŸ’¥ Error: {e}")

def test_course_generation(api_key):
    """Test the actual course generation"""
    
    prompt = """Create a course structure for: Python for Data Science

Student Profile:
- Level: beginner
- Style Preference: practical
- Total Time: 10 hours
- Daily Time: 30 minutes per day

Create a modular course where each module is 30-60 minutes.

Return JSON with this structure:
{
  "title": "Course Title",
  "total_modules": 5,
  "modules": [
    {
      "id": "mod_1",
      "title": "Module Title", 
      "learning_objectives": ["objective1", "objective2"],
      "topics": ["topic1", "topic2"],
      "estimated_minutes": 45
    }
  ]
}"""

    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json"
    }
    
    payload = {
        "model": "openai/gpt-oss-20b:free",
        "messages": [
            {"role": "system", "content": "You are a course designer. Return only valid JSON."},
            {"role": "user", "content": prompt}
        ],
        "temperature": 0.3
    }
    
    try:
        response = requests.post(
            "https://openrouter.ai/api/v1/chat/completions",
            headers=headers,
            json=payload,
            timeout=60
        )
        
        if response.status_code == 200:
            result = response.json()
            content = result["choices"][0]["message"]["content"]
            
            print("âœ… Course Generation SUCCESS!")
            print("Raw response:")
            print("-" * 50)
            print(content)
            print("-" * 50)
            
            # Try to parse JSON
            try:
                if "```json" in content:
                    content = content.split("```json")[1].split("```")[0]
                elif "```" in content:
                    content = content.split("```")[1].split("```")[0]
                
                course_data = json.loads(content.strip())
                print("ðŸŽ‰ JSON Parse SUCCESS!")
                print(f"ðŸ“š Title: {course_data.get('title')}")
                print(f"ðŸ“¦ Modules: {course_data.get('total_modules')}")
                
            except json.JSONDecodeError as e:
                print(f"âš ï¸ JSON parse failed: {e}")
                print("But the API call worked!")
                
        else:
            print(f"âŒ Course generation failed: {response.status_code}")
            print(response.text)
            
    except Exception as e:
        print(f"ðŸ’¥ Course generation error: {e}")

if __name__ == "__main__":
    test_openrouter()
----------------------------------------
File: ./api/routes/toc_routes.py
----------------------------------------
# ============================================================================
# File: api/routes/toc_routes.py
# API routes for TOC generation and learning path creation
# ============================================================================

from fastapi import APIRouter, HTTPException, Depends
from typing import Dict, Any, List
from pydantic import BaseModel, Field
import uuid
import logging

from generators.toc_generator import TOCGenerator
from services.learning_path_service import learning_path_service
from models.toc_models import TOCResponse, LearningPathResponse
from domains.data_science.prompts.toc_prompts import get_toc_prompt

logger = logging.getLogger(__name__)

router = APIRouter(prefix="/api/toc", tags=["Table of Contents"])

# Request models
class TOCRequest(BaseModel):
    domain: str = Field(..., description="Domain for curriculum (e.g., 'data_science')")
    user_preferences: Dict[str, Any] = Field(..., description="User preferences from onboarding")

class LearningPathRequest(BaseModel):
    session_id: str = Field(..., description="Session ID from TOC generation")
    user_id: str = Field(..., description="User identifier")
    selected_topic_ids: List[str] = Field(..., description="List of selected topic IDs")

class LearningPathUpdateRequest(BaseModel):
    session_id: str = Field(..., description="Session ID")
    selected_topic_ids: List[str] = Field(..., description="Updated list of selected topic IDs")

# Dependency to get TOC generator
def get_toc_generator() -> TOCGenerator:
    return TOCGenerator()

@router.post("/generate", response_model=TOCResponse)
async def generate_toc(
    request: TOCRequest,
    toc_generator: TOCGenerator = Depends(get_toc_generator)
):
    """
    Generate Table of Contents based on domain and user preferences
    """
    try:
        session_id = str(uuid.uuid4())
        
        logger.info(f"Generating TOC for domain: {request.domain}")
        logger.info(f"Session ID: {session_id}")
        
        # Get domain-specific prompt
        if request.domain == "data_science":
            domain_prompt = get_toc_prompt(request.user_preferences)
        else:
            raise HTTPException(
                status_code=400, 
                detail=f"Domain '{request.domain}' not supported yet"
            )
        
        # Generate TOC
        toc = toc_generator.generate_toc(
            domain=request.domain,
            user_preferences=request.user_preferences,
            domain_prompt=domain_prompt
        )
        
        # Store TOC in learning path service
        learning_path_service.store_toc(session_id, toc)
        
        logger.info(f"Successfully generated TOC with {len(toc.topics)} topics")
        
        return TOCResponse(
            data=toc,
            message=f"TOC generated successfully for {request.domain}"
        )
        
    except Exception as e:
        logger.error(f"TOC generation failed: {str(e)}")
        raise HTTPException(status_code=500, detail=f"TOC generation failed: {str(e)}")

@router.get("/session/{session_id}")
async def get_toc_by_session(session_id: str):
    """
    Retrieve TOC by session ID
    """
    try:
        toc = learning_path_service.get_toc(session_id)
        if not toc:
            raise HTTPException(status_code=404, detail="TOC not found")
        
        return TOCResponse(
            data=toc,
            message="TOC retrieved successfully"
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error retrieving TOC: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@router.post("/learning-path", response_model=LearningPathResponse)
async def create_learning_path(request: LearningPathRequest):
    """
    Create learning path from selected topics
    """
    try:
        logger.info(f"Creating learning path for user {request.user_id}")
        logger.info(f"Selected topics: {request.selected_topic_ids}")
        
        learning_path = learning_path_service.create_learning_path(
            user_id=request.user_id,
            session_id=request.session_id,
            selected_topic_ids=request.selected_topic_ids
        )
        
        return LearningPathResponse(
            data=learning_path,
            message=f"Learning path created with {len(request.selected_topic_ids)} topics"
        )
        
    except ValueError as e:
        logger.error(f"Invalid learning path request: {str(e)}")
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        logger.error(f"Learning path creation failed: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@router.put("/learning-path", response_model=LearningPathResponse)
async def update_learning_path(request: LearningPathUpdateRequest):
    """
    Update existing learning path with new topic selection
    """
    try:
        logger.info(f"Updating learning path for session {request.session_id}")
        
        learning_path = learning_path_service.update_learning_path(
            session_id=request.session_id,
            selected_topic_ids=request.selected_topic_ids
        )
        
        return LearningPathResponse(
            data=learning_path,
            message=f"Learning path updated with {len(request.selected_topic_ids)} topics"
        )
        
    except ValueError as e:
        logger.error(f"Invalid learning path update: {str(e)}")
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        logger.error(f"Learning path update failed: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/learning-path/{session_id}")
async def get_learning_path(session_id: str):
    """
    Get learning path by session ID
    """
    try:
        learning_path = learning_path_service.get_learning_path(session_id)
        if not learning_path:
            raise HTTPException(status_code=404, detail="Learning path not found")
        
        return LearningPathResponse(
            data=learning_path,
            message="Learning path retrieved successfully"
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error retrieving learning path: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/user/{user_id}/learning-paths")
async def get_user_learning_paths(user_id: str):
    """
    Get all learning paths for a user
    """
    try:
        learning_paths = learning_path_service.get_user_learning_paths(user_id)
        
        return {
            "success": True,
            "data": learning_paths,
            "count": len(learning_paths),
            "message": f"Retrieved {len(learning_paths)} learning paths"
        }
        
    except Exception as e:
        logger.error(f"Error retrieving user learning paths: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/topic/{session_id}/{topic_id}")
async def get_topic_details(session_id: str, topic_id: str):
    """
    Get detailed information about a specific topic
    """
    try:
        topic_details = learning_path_service.get_topic_details(session_id, topic_id)
        if not topic_details:
            raise HTTPException(status_code=404, detail="Topic not found")
        
        return {
            "success": True,
            "data": topic_details,
            "message": "Topic details retrieved successfully"
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error retrieving topic details: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/statistics")
async def get_toc_statistics():
    """
    Get TOC service statistics
    """
    try:
        stats = learning_path_service.get_statistics()
        return {
            "success": True,
            "data": stats,
            "message": "Statistics retrieved successfully"
        }
        
    except Exception as e:
        logger.error(f"Error retrieving statistics: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))
----------------------------------------
File: ./generators/toc_generator.py
----------------------------------------
# ============================================================================
# File: generators/toc_generator.py
# Completely generic TOC generator - no opinions, just passes data to LLM
# ============================================================================

from typing import Dict, Any
from services.llm_service import LLMService
from models.toc_models import TableOfContents
import logging

logger = logging.getLogger(__name__)

class TOCGenerator:
    def __init__(self, model: str = "openai/gpt-oss-20b:free"):
        self.llm = LLMService(model=model)
    
    def generate_toc(self, 
                     domain: str, 
                     user_preferences: Dict[str, Any], 
                     domain_prompt: str) -> TableOfContents:
        """
        Generate Table of Contents using domain-specific prompt
        
        This is a completely generic wrapper - it doesn't make any decisions
        about the curriculum. All logic is delegated to the LLM via the domain prompt.
        
        Args:
            domain: Domain identifier (e.g., 'data_science', 'cat_exam')
            user_preferences: Complete user preferences from onboarding
            domain_prompt: Domain-specific prompt template
            
        Returns:
            TableOfContents: Structured TOC generated by LLM
        """
        
        # Simple system prompt that doesn't bias the LLM
        system_prompt = f"""You are an expert curriculum designer for {domain}. 
        
        Your job is to create a personalized Table of Contents that perfectly matches 
        the user's stated preferences and goals. 
        
        Be completely responsive to their:
        - Learning goals and objectives
        - Experience level and background
        - Time constraints and availability  
        - Learning style preferences
        - Technical background
        
        Do not impose any predetermined structure or bias. Let their preferences 
        guide every decision about topics, depth, difficulty, and focus areas.
        
        Generate a comprehensive yet personalized curriculum that they will actually 
        want to follow and complete.
        
        Return valid JSON that matches the required schema exactly."""
        
        logger.info(f"Generating TOC for domain: {domain}")
        logger.debug(f"User preferences keys: {list(user_preferences.keys())}")
        
        try:
            toc = self.llm.generate_structured(
                system_prompt=system_prompt,
                user_prompt=domain_prompt,
                response_model=TableOfContents,
                temperature=0.7  # Allow creativity while staying structured
            )
            
            logger.info(f"Successfully generated TOC with {len(toc.topics)} topics")
            logger.info(f"Total estimated hours: {toc.total_estimated_hours}")
            
            return toc
            
        except Exception as e:
            logger.error(f"Failed to generate TOC: {str(e)}")
            raise Exception(f"TOC generation failed: {str(e)}")
    
    def set_model(self, model: str):
        """Switch LLM model"""
        self.llm.set_model(model)
        logger.info(f"TOC Generator switched to model: {model}")
----------------------------------------
File: ./main.py
----------------------------------------
# ============================================================================
# File: main.py
# Updated FastAPI app with new TOC generation architecture
# ============================================================================

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
import logging
import os
from dotenv import load_dotenv

# Import new route modules
from api.routes.toc_routes import router as toc_router

# Load environment variables
load_dotenv()

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Create FastAPI app
app = FastAPI(
    title="Learning Platform API",
    description="Modular learning platform with domain-specific curriculum generation",
    version="3.0.0"
)

# Enable CORS for React frontend
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000", "http://127.0.0.1:3000"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Include routers
app.include_router(toc_router)

@app.get("/")
async def root():
    """Root endpoint"""
    return {
        "message": "Learning Platform API v3.0",
        "features": [
            "Modular TOC generation",
            "Domain-specific prompts", 
            "Structured LLM output",
            "Learning path management"
        ]
    }

@app.get("/api/health")
async def health_check():
    """Health check endpoint"""
    
    # Check if OpenRouter API key is configured
    api_key = os.getenv("OPENROUTER_API_KEY")
    api_configured = bool(api_key and api_key != "your-key-here")
    
    return {
        "status": "healthy",
        "version": "3.0.0",
        "api_configured": api_configured,
        "features": {
            "toc_generation": True,
            "learning_path_management": True,
            "structured_output": True,
            "domain_prompts": True
        }
    }

@app.get("/api/domains")
async def get_supported_domains():
    """Get list of supported domains"""
    return {
        "success": True,
        "data": {
            "supported_domains": [
                {
                    "id": "data_science",
                    "name": "Data Science",
                    "description": "Comprehensive data science curriculum with ML, statistics, and programming",
                    "status": "active"
                }
            ],
            "coming_soon": [
                {
                    "id": "cat_exam", 
                    "name": "CAT Exam Preparation",
                    "description": "Quantitative Aptitude, Verbal Ability, and Logical Reasoning",
                    "status": "development"
                }
            ]
        },
        "message": "Supported domains retrieved successfully"
    }

# Global exception handler
@app.exception_handler(Exception)
async def global_exception_handler(request, exc):
    logger.error(f"Global exception: {str(exc)}")
    return HTTPException(
        status_code=500,
        detail=f"Internal server error: {str(exc)}"
    )

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(
        "main:app",  # Import string instead of app object
        host="0.0.0.0", 
        port=8000, 
        log_level="info",
        reload=True  # Enable auto-reload during development
    )
----------------------------------------
File: ./content_generator.py
----------------------------------------
from typing import List, Dict
import uuid
from models import *
from backend.services.llm_service import LLMService
from prompts import *

class ContentGenerator:
    def __init__(self):
        self.llm = LLMService()
    
    def generate_course_structure(self, preferences: UserPreferences) -> Dict:
        """Generate complete course structure"""
        prompt = COURSE_STRUCTURE_PROMPT.format(
            topic=preferences.topic,
            level=preferences.learner_level,
            style=preferences.course_style,
            total_hours=preferences.total_hours,
            daily_minutes=preferences.daily_minutes
        )
        
        structure = self.llm.generate_json(
            "You are an expert curriculum designer for data science.",
            prompt
        )
        
        # Calculate number of sessions
        total_minutes = preferences.total_hours * 60
        sessions = total_minutes // preferences.daily_minutes
        
        print(f"Generated course with {len(structure['modules'])} modules for {sessions} sessions")
        return structure
    
    def generate_module_content(self, module: Dict, preferences: UserPreferences) -> Module:
        """Generate content for a single module"""
        # Calculate theory vs code percentages
        style_map = {
            CourseStyle.HANDS_ON: (20, 80),
            CourseStyle.BALANCED: (40, 60),
            CourseStyle.CONCEPT: (60, 40)
        }
        theory_percent, code_percent = style_map[preferences.course_style]
        
        prompt = MODULE_CONTENT_PROMPT.format(
            style=preferences.course_style,
            module_title=module['title'],
            topics=", ".join(module.get('topics', [])),
            objectives=", ".join(module.get('learning_objectives', [])),
            duration=module['estimated_minutes'],
            level=preferences.learner_level,
            theory_percent=theory_percent,
            code_percent=code_percent
        )
        
        content = self.llm.generate(
            "You are an expert data science instructor.",
            prompt
        )
        
        # Generate exercises for the module
        exercises = self.generate_exercises(
            module['title'], 
            preferences.learner_level
        )
        
        return Module(
            id=module['id'],
            title=module['title'],
            content=content,
            exercises=exercises,
            estimated_minutes=module['estimated_minutes'],
            order=module.get('order', 0)
        )
    
    def generate_exercises(self, topic: str, level: str, count: int = 2) -> List[Dict]:
        """Generate exercises for a topic"""
        exercises = []
        difficulty_map = {
            LearnerLevel.BEGINNER: ["easy", "easy"],
            LearnerLevel.INTERMEDIATE: ["easy", "medium"],
            LearnerLevel.ADVANCED: ["medium", "hard"]
        }
        
        for difficulty in difficulty_map.get(level, ["easy", "medium"]):
            prompt = EXERCISE_GENERATOR_PROMPT.format(
                topic=topic,
                difficulty=difficulty,
                focus_area="practical application"
            )
            
            exercise = self.llm.generate_json(
                "You are a coding exercise designer.",
                prompt
            )
            exercise['id'] = str(uuid.uuid4())[:8]
            exercises.append(exercise)
        
        return exercises
    
    def regenerate_exercise(self, topic: str, difficulty: str) -> Dict:
        """Generate a new exercise variant"""
        prompt = EXERCISE_GENERATOR_PROMPT.format(
            topic=topic,
            difficulty=difficulty,
            focus_area="different approach"
        )
        
        exercise = self.llm.generate_json(
            "Generate a NEW, different exercise. Don't repeat previous patterns.",
            prompt
        )
        exercise['id'] = str(uuid.uuid4())[:8]
        return exercise
----------------------------------------
File: ./llm_repo_digest.sh
----------------------------------------
#!/bin/bash

# Get repository name and set up output file
REPO_NAME=$(basename "$PWD")
OUTPUT_FILE="${REPO_NAME}_repo_digest.txt"

# Define source code file extensions we want to include
SOURCE_EXTENSIONS="\.(py|ipynb|js|jsx|ts|tsx|vue|java|cpp|hpp|c|h|go|rs|rb|php|cs|scala|kt|swift|m|mm|sh|bash|pl|pm|t|less|html|xml|sql|graphql|md|rst|tex|yaml|yml|json|coffee|dart|r|jl|lua|clj|cljs|ex|exs)$"

# Define common patterns to exclude
EXCLUDE_PATTERNS=(
    # Version control
    ".git"
    "__pycache__"
    
    # Data and binary files
    "*.csv"
    "*.xlsx"
    "*.json"
    "*.log"
    
    # Build and environment
    "node_modules"
    "docker"
    "venv"
    ".env"
    
    # IDE and editor files
    ".vscode"
    ".idea"
    "*.swp"
)

# Helper function to check if a file is binary
is_binary() {
    [ ! -f "$1" ] && return 1
    local mime
    mime=$(file -b --mime "$1")
    case "$mime" in
        *binary*) return 0 ;;
        *charset=binary*) return 0 ;;
        *) return 1 ;;
    esac
}

# Build exclude arguments for find command
build_find_excludes() {
    local excludes=()
    
    # Process predefined exclude patterns
    for pat in "${EXCLUDE_PATTERNS[@]}"; do
        if [[ "$pat" == *[*?]* ]]; then
            # Pattern contains wildcards - use -name
            excludes+=("-name" "$pat" "-prune" "-o")
        else
            # No wildcards - use -path
            excludes+=("-path" "./$pat" "-prune" "-o")
        fi
    done
    
    # Add patterns from .gitignore if it exists
    if [ -f .gitignore ]; then
        while IFS= read -r pattern; do
            # Skip comments and empty lines
            [[ "$pattern" =~ ^#.*$ || -z "$pattern" ]] && continue
            
            # Clean up pattern: remove trailing and leading slashes
            pattern="${pattern%/}"
            pattern="${pattern#/}"
            [[ -n "$pattern" ]] || continue
            
            # Handle wildcards in gitignore patterns
            if [[ "$pattern" == *[*?]* ]]; then
                excludes+=("-name" "$pattern" "-prune" "-o")
            else
                excludes+=("-path" "./$pattern" "-prune" "-o")
            fi
        done < .gitignore
    fi
    
    printf '%s\n' "${excludes[@]}"
}

# Initialize output file
> "$OUTPUT_FILE"
echo "Repository Source Code Contents" >> "$OUTPUT_FILE"
echo "Generated on: $(date)" >> "$OUTPUT_FILE"
echo "----------------------------------------" >> "$OUTPUT_FILE"

# Initialize counters
total_files=0
included_files=0
excluded_binary=0

echo "Building find command..." >&2

# Build the find command with excludes
mapfile -t FIND_EXCLUDES < <(build_find_excludes)

# Process files using find with proper array expansion
while IFS= read -r -d $'\0' path; do
    ((total_files++))
    echo "Processing: $path"
    
    if [[ "$path" =~ $SOURCE_EXTENSIONS ]]; then
        if ! is_binary "$path"; then
            echo "File: $path" >> "$OUTPUT_FILE"
            echo "----------------------------------------" >> "$OUTPUT_FILE"
            cat "$path" >> "$OUTPUT_FILE"
            echo -e "\n----------------------------------------" >> "$OUTPUT_FILE"
            ((included_files++))
        else
            echo "Skipping binary: $path"
            ((excluded_binary++))
        fi
    else
        echo "Skipping non-source file: $path"
    fi
done < <(find . "${FIND_EXCLUDES[@]}" -type f -print0)

# Print summary
echo -e "\nSummary:"
echo "Total files found: $total_files"
echo "Included in output: $included_files"
echo "Skipped binary files: $excluded_binary"
echo "Output: $OUTPUT_FILE"
----------------------------------------
File: ./services/learning_path_service.py
----------------------------------------
# ============================================================================
# File: services/learning_path_service.py
# Service to manage learning paths with in-memory storage
# ============================================================================

from typing import Dict, List, Any, Optional
from models.toc_models import LearningPath, TableOfContents
import uuid
from datetime import datetime
import logging

logger = logging.getLogger(__name__)

class LearningPathService:
    def __init__(self):
        # In-memory storage for learning paths and TOCs
        self.learning_paths: Dict[str, LearningPath] = {}
        self.tocs: Dict[str, TableOfContents] = {}
        self.user_sessions: Dict[str, List[str]] = {}  # user_id -> list of session_ids
    
    def store_toc(self, session_id: str, toc: TableOfContents) -> None:
        """Store generated TOC for a session"""
        self.tocs[session_id] = toc
        logger.info(f"Stored TOC for session {session_id} with {len(toc.topics)} topics")
    
    def get_toc(self, session_id: str) -> Optional[TableOfContents]:
        """Retrieve TOC for a session"""
        return self.tocs.get(session_id)
    
    def create_learning_path(self, 
                           user_id: str,
                           session_id: str,
                           selected_topic_ids: List[str],
                           user_preferences: Optional[Dict[str, Any]] = None) -> LearningPath:
        """
        Create a learning path from selected topics
        
        Args:
            user_id: User identifier
            session_id: Session identifier 
            selected_topic_ids: List of topic IDs selected by user
            user_preferences: User preferences from onboarding
            
        Returns:
            LearningPath: Created learning path object
        """
        
        # Get the TOC for this session
        toc = self.get_toc(session_id)
        if not toc:
            raise ValueError(f"No TOC found for session {session_id}")
        
        # Validate selected topics exist in TOC
        toc_topic_ids = {topic.id for topic in toc.topics}
        invalid_topics = set(selected_topic_ids) - toc_topic_ids
        if invalid_topics:
            raise ValueError(f"Invalid topic IDs: {invalid_topics}")
        
        # Calculate total estimated hours for selected topics
        selected_topics = [topic for topic in toc.topics if topic.id in selected_topic_ids]
        total_hours = sum(topic.estimated_hours for topic in selected_topics)
        
        # Create learning path
        learning_path = LearningPath(
            user_id=user_id,
            session_id=session_id,
            domain=toc.domain,
            selected_topics=selected_topic_ids,
            estimated_total_hours=total_hours,
            created_at=datetime.now().isoformat(),
            user_preferences=user_preferences
        )
        
        # Store learning path
        self.learning_paths[session_id] = learning_path
        
        # Track user sessions
        if user_id not in self.user_sessions:
            self.user_sessions[user_id] = []
        self.user_sessions[user_id].append(session_id)
        
        logger.info(f"Created learning path for user {user_id}, session {session_id}")
        logger.info(f"Selected {len(selected_topic_ids)} topics, estimated {total_hours:.1f} hours")
        
        return learning_path
    
    def get_learning_path(self, session_id: str) -> Optional[LearningPath]:
        """Get learning path by session ID"""
        return self.learning_paths.get(session_id)
    
    def get_user_learning_paths(self, user_id: str) -> List[LearningPath]:
        """Get all learning paths for a user"""
        session_ids = self.user_sessions.get(user_id, [])
        return [self.learning_paths[sid] for sid in session_ids if sid in self.learning_paths]
    
    def update_learning_path(self, 
                           session_id: str, 
                           selected_topic_ids: List[str]) -> LearningPath:
        """
        Update an existing learning path with new topic selection
        """
        learning_path = self.get_learning_path(session_id)
        if not learning_path:
            raise ValueError(f"No learning path found for session {session_id}")
        
        # Get TOC and validate topics
        toc = self.get_toc(session_id)
        if not toc:
            raise ValueError(f"No TOC found for session {session_id}")
        
        toc_topic_ids = {topic.id for topic in toc.topics}
        invalid_topics = set(selected_topic_ids) - toc_topic_ids
        if invalid_topics:
            raise ValueError(f"Invalid topic IDs: {invalid_topics}")
        
        # Recalculate total hours
        selected_topics = [topic for topic in toc.topics if topic.id in selected_topic_ids]
        total_hours = sum(topic.estimated_hours for topic in selected_topics)
        
        # Update learning path
        learning_path.selected_topics = selected_topic_ids
        learning_path.estimated_total_hours = total_hours
        
        logger.info(f"Updated learning path for session {session_id}")
        logger.info(f"New selection: {len(selected_topic_ids)} topics, {total_hours:.1f} hours")
        
        return learning_path
    
    def get_topic_details(self, session_id: str, topic_id: str) -> Optional[Dict[str, Any]]:
        """Get detailed information about a specific topic"""
        toc = self.get_toc(session_id)
        if not toc:
            return None
        
        for topic in toc.topics:
            if topic.id == topic_id:
                return {
                    "topic": topic.dict(),
                    "prerequisites": [
                        self._get_topic_by_id(toc, prereq_id) 
                        for prereq_id in topic.prerequisites
                    ],
                    "dependents": [
                        t.dict() for t in toc.topics 
                        if topic_id in t.prerequisites
                    ]
                }
        return None
    
    def _get_topic_by_id(self, toc: TableOfContents, topic_id: str) -> Optional[Dict[str, Any]]:
        """Helper to get topic by ID"""
        for topic in toc.topics:
            if topic.id == topic_id:
                return topic.dict()
        return None
    
    def get_statistics(self) -> Dict[str, Any]:
        """Get service statistics"""
        return {
            "total_tocs": len(self.tocs),
            "total_learning_paths": len(self.learning_paths),
            "total_users": len(self.user_sessions),
            "average_topics_per_path": (
                sum(len(lp.selected_topics) for lp in self.learning_paths.values()) / 
                len(self.learning_paths) if self.learning_paths else 0
            )
        }

# Global service instance
learning_path_service = LearningPathService()
----------------------------------------
File: ./services/llm_service.py
----------------------------------------
# ============================================================================
# File: services/llm_service.py
# Enhanced LLM service with OpenRouter structured output support
# ============================================================================

import requests
import json
import os
from typing import Dict, Any, Optional, Type
from pydantic import BaseModel
from tenacity import retry, wait_exponential, stop_after_attempt
import logging

from dotenv import load_dotenv

load_dotenv()

logger = logging.getLogger(__name__)

class LLMService:
    def __init__(self, model: str = "openai/gpt-oss-20b:free"):
        self.api_key = os.getenv("OPENROUTER_API_KEY")
        if not self.api_key:
            raise ValueError("OPENROUTER_API_KEY environment variable is required")
        
        self.base_url = "https://openrouter.ai/api/v1"
        self.model = model
        
    @retry(wait=wait_exponential(min=1, max=10), stop=stop_after_attempt(3))
    def generate(self, system_prompt: str, user_prompt: str, temperature: float = 0.7) -> str:
        """Basic LLM generation for unstructured output"""
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",
            "HTTP-Referer": "https://learning-platform.com",
            "X-Title": "Learning Platform"
        }
        
        payload = {
            "model": self.model,
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            "temperature": temperature
        }
        
        response = requests.post(
            f"{self.base_url}/chat/completions",
            headers=headers,
            json=payload
        )
        
        if response.status_code != 200:
            raise Exception(f"API request failed: {response.status_code} - {response.text}")
        
        result = response.json()
        return result["choices"][0]["message"]["content"]
    
    @retry(wait=wait_exponential(min=1, max=10), stop=stop_after_attempt(3))
    def generate_structured(self, 
                          system_prompt: str, 
                          user_prompt: str, 
                          response_model: Type[BaseModel],
                          temperature: float = 0.3) -> BaseModel:
        """Generate structured output using OpenRouter's JSON schema feature"""
        
        # Generate JSON schema from Pydantic model
        schema = response_model.model_json_schema()
        
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",
            "HTTP-Referer": "https://learning-platform.com",
            "X-Title": "Learning Platform"
        }
        
        payload = {
            "model": self.model,
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            "temperature": temperature,
            "response_format": {
                "type": "json_schema",
                "json_schema": {
                    "name": response_model.__name__.lower(),
                    "strict": True,
                    "schema": schema
                }
            }
        }
        
        logger.info(f"Making structured request to {self.model}")
        logger.debug(f"Schema: {schema}")
        
        response = requests.post(
            f"{self.base_url}/chat/completions",
            headers=headers,
            json=payload
        )
        
        if response.status_code != 200:
            raise Exception(f"Structured API request failed: {response.status_code} - {response.text}")
        
        result = response.json()
        content = result["choices"][0]["message"]["content"]
        
        # Parse and validate with Pydantic
        try:
            data = json.loads(content)
            return response_model(**data)
        except (json.JSONDecodeError, ValueError) as e:
            logger.error(f"Failed to parse structured response: {content}")
            raise Exception(f"Failed to parse structured response: {e}")
    
    def set_model(self, model: str):
        """Switch models on the fly"""
        self.model = model
        logger.info(f"Switched to model: {model}")
----------------------------------------
